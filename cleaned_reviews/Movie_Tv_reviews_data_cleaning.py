# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZW3PPVqf3HJQtqJYPwV8f3iRk3B1eh9P
"""

!pip install --quiet gdown

import gdown
import pandas as pd
import re
import nltk
import json
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# Step 1: Download the CSV file from Google Drive
file_id = '1DhMHXChQll5iO4JDYlfWBUruzo7PzIlW'
url = f'https://drive.google.com/uc?id={file_id}'
output = 'downloaded_file.csv'
gdown.download(url, output, quiet=False)

# Step 2: Read the CSV into a DataFrame
df = pd.read_csv(output)

# Step 3: Download NLTK stopwords
nltk.download('stopwords')

# Step 4: Initialize the stemmer and stopwords once
ps = PorterStemmer()
all_stopwords = stopwords.words('english')
if 'not' in all_stopwords:
    all_stopwords.remove('not')

# Step 5: Clean all reviewText entries
corpus = []
for review in df['reviewText'].fillna(''):
    review = re.sub('[^a-zA-Z]', ' ', review)  # Keep only letters
    review = review.lower()
    words = review.split()
    words = [ps.stem(word) for word in words if word not in set(all_stopwords)]
    cleaned_review = ' '.join(words)
    corpus.append(cleaned_review)

# Step 6: Create new DataFrame with asin and cleaned reviews only
cleaned_df = pd.DataFrame({
    'asin': df['asin'],
    'cleanedReview': corpus
})

# Step 7: Save cleaned data to CSV and JSON files
csv_output = 'cleaned_reviews.csv'
json_output = 'cleaned_reviews.json'

cleaned_df.to_csv(csv_output, index=False)
cleaned_df.to_json(json_output, orient='records', force_ascii=False, indent=4)

print(f"Cleaned data saved to:\n - {csv_output}\n - {json_output}")